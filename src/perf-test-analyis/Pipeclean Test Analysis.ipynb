{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e097c57",
   "metadata": {},
   "source": [
    "# Pipeclean Test Analysis\n",
    "\n",
    "## Setup\n",
    "\n",
    "Please run the following cell, enter the path to the performance test results, and click the Run button. Default value matches the `.env.example` file in the root of this repository. If you enter a path with multiple results, this notebook will analyze the most recent set of test results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3a281365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ec43b6a291e42d08ac1184ed120b227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='../../testResults', description='Results Path:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21798ab8a04a4ed68675f16e09bc373f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='primary', description='Run', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb343164954248938ba2d391318b63fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "import pandas as pd\n",
    "\n",
    "from edfi_perf_test_analysis.ui_helpers import log_error, log_info, markdown, log_warning\n",
    "from os import scandir, path\n",
    "        \n",
    "\n",
    "from typing import Optional\n",
    "def display_df(df: pd.DataFrame, max_rows: Optional[int] = None) -> None:\n",
    "    \n",
    "    if df.shape[0] == 0:\n",
    "        log_warning(\"No data to display\")\n",
    "        return\n",
    "    \n",
    "    display(HTML(\n",
    "      df.fillna(\"\")\n",
    "        .style\n",
    "        .format(precision=2)\n",
    "        .set_properties(**{'text-align': 'left'})\n",
    "        .set_table_styles([dict(selector='th', props=[('text-align', 'left')])])\n",
    "        .to_html(index=False, max_rows=max_rows)\n",
    "    ))\n",
    "\n",
    "\n",
    "\n",
    "results_path = widgets.Text(value=\"../../testResults\", description=\"Results Path:\")\n",
    "run = widgets.Button(\n",
    "    description=\"Run\", button_style=\"primary\"\n",
    ")\n",
    "\n",
    "output: widgets.Output = widgets.Output()\n",
    "display(results_path, run, output)\n",
    "\n",
    "def on_run(button) -> None:\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        file_path = results_path.value\n",
    "        \n",
    "        if not path.exists(file_path):\n",
    "            log_error(f\"Directory `{file_path}` does not exist or could not be read.\")\n",
    "            return       \n",
    "        \n",
    "        directories = sorted([\n",
    "            f.path\n",
    "            for f in scandir(file_path)\n",
    "            if f.is_dir()\n",
    "        ])\n",
    "        \n",
    "        results_dir: str\n",
    "        if len(directories) == 0:\n",
    "            # Assume this directory has the test results\n",
    "            results_dir = file_path\n",
    "        else:\n",
    "            # Sorted oldest to newest, so analyze the _last_ item as newest\n",
    "            results_dir = directories[-1]\n",
    "        \n",
    "        log_info(results_dir)\n",
    "        \n",
    "        \n",
    "        exceptions = pd.read_csv(path.join(results_dir, \"pipeclean_exceptions.csv\"))\n",
    "        exceptions.set_index(\"Message\", inplace=True)\n",
    "        \n",
    "        failures = pd.read_csv(path.join(results_dir, \"pipeclean_failures.csv\"))\n",
    "        failures.set_index(\"Name\", inplace=True)\n",
    "        \n",
    "        stats = pd.read_csv(path.join(results_dir, \"pipeclean_stats.csv\"))\n",
    "        \n",
    "        # Remove the \"Aggregated\" row\n",
    "        stats.drop(stats.index[stats[\"Name\"] == \"Aggregated\"], inplace=True)\n",
    "        \n",
    "        # Create a new column combining Type and Name, then index on it\n",
    "        stats[\"Request\"] = stats[\"Type\"] + \" \" + stats[\"Name\"]                \n",
    "        stats.set_index(\"Request\", inplace=True)\n",
    "        \n",
    "        stats = stats[[\"Average Response Time\", \"Request Count\", \"Failure Count\"]]\n",
    "        stats.rename(columns={\"Average Response Time\": \"Response Time\"}, inplace=True)\n",
    "                \n",
    "        \n",
    "        markdown(\"### Exceptions\")\n",
    "        markdown(\"First five exceptions\")\n",
    "        display_df(exceptions, 5)\n",
    "\n",
    "        markdown(\"### Failures\")\n",
    "        markdown(\"First five failures\")\n",
    "        display_df(failures, 5)\n",
    "                \n",
    "        markdown(\"### Summary Stats\")\n",
    "        if (stats.shape[1] > 0):\n",
    "            summary_stats = stats.aggregate(\n",
    "                {\n",
    "                    \"Request Count\": [\"sum\"],\n",
    "                    \"Failure Count\": [\"sum\"],\n",
    "                    \"Response Time\": [\"mean\", \"min\", \"max\"]\n",
    "                }\n",
    "            )\n",
    "            display_df(summary_stats)\n",
    "            \n",
    "        markdown(\"### Ten Worst Average Response Times\")\n",
    "        display_df(stats.sort_values(by=[\"Response Time\"], ascending=False), 10)\n",
    "        \n",
    "        markdown(\"### Ten Best Average Response Times\")\n",
    "        display_df(stats.sort_values(by=[\"Response Time\"], ascending=True), 10)\n",
    "        \n",
    "\n",
    "        \n",
    "run.on_click(on_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1ad7822c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46d63c69878e40b6a0aa4db80a5511aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='../../testResults', description='Results Path:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21a5976f8858447b85a8e7b8305e7798",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='primary', description='Run', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b3dc86e3d044cc59e850423aceca1bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from edfi_perf_test_analysis.ui_helpers import select_dir_and_run\n",
    "\n",
    "def pipeclean_analysis(results_dir: str) -> None:\n",
    "    exceptions = pd.read_csv(path.join(results_dir, \"pipeclean_exceptions.csv\"))\n",
    "    exceptions.set_index(\"Message\", inplace=True)\n",
    "\n",
    "    failures = pd.read_csv(path.join(results_dir, \"pipeclean_failures.csv\"))\n",
    "    failures.set_index(\"Name\", inplace=True)\n",
    "\n",
    "    stats = pd.read_csv(path.join(results_dir, \"pipeclean_stats.csv\"))\n",
    "\n",
    "    # Remove the \"Aggregated\" row\n",
    "    stats.drop(stats.index[stats[\"Name\"] == \"Aggregated\"], inplace=True)\n",
    "\n",
    "    # Create a new column combining Type and Name, then index on it\n",
    "    stats[\"Request\"] = stats[\"Type\"] + \" \" + stats[\"Name\"]                \n",
    "    stats.set_index(\"Request\", inplace=True)\n",
    "\n",
    "    stats = stats[[\"Average Response Time\", \"Request Count\", \"Failure Count\"]]\n",
    "    stats.rename(columns={\"Average Response Time\": \"Response Time\"}, inplace=True)\n",
    "\n",
    "\n",
    "    markdown(\"### Exceptions\")\n",
    "    markdown(\"First five exceptions\")\n",
    "    display_df(exceptions, 5)\n",
    "\n",
    "    markdown(\"### Failures\")\n",
    "    markdown(\"First five failures\")\n",
    "    display_df(failures, 5)\n",
    "\n",
    "    markdown(\"### Summary Stats\")\n",
    "    if (stats.shape[1] > 0):\n",
    "        summary_stats = stats.aggregate(\n",
    "            {\n",
    "                \"Request Count\": [\"sum\"],\n",
    "                \"Failure Count\": [\"sum\"],\n",
    "                \"Response Time\": [\"mean\", \"min\", \"max\"]\n",
    "            }\n",
    "        )\n",
    "        display_df(summary_stats)\n",
    "\n",
    "    markdown(\"### Ten Worst Average Response Times\")\n",
    "    display_df(stats.sort_values(by=[\"Response Time\"], ascending=False), 10)\n",
    "\n",
    "    markdown(\"### Ten Best Average Response Times\")\n",
    "    display_df(stats.sort_values(by=[\"Response Time\"], ascending=True), 10)\n",
    "    \n",
    "select_dir_and_run(pipeclean_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d58ffc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
